**ğŸ“„ Chat with Multiple PDFs using Gemini (Streamlit + LangChain)**

An interactive AI-powered PDF Question Answering application built using Streamlit, LangChain, FAISS, and Google Gemini.
Users can upload multiple PDF documents and ask natural language questions to retrieve accurate answers directly from the document content.


**ğŸš€ Features**

ğŸ“‚ Upload multiple PDF files

ğŸ” Extract and process text from PDFs

ğŸ§  Convert text into vector embeddings

âš¡ Store embeddings using FAISS vector database

ğŸ¤– Ask questions using Google Gemini (LLM)

ğŸ’¬ Chat-like interface with conversation history

ğŸ” Secure API key management using .env




**ğŸ› ï¸ Tech Stack**


| Category               | Technology                       |
| ---------------------- | -------------------------------- |
| Frontend               | Streamlit                        |
| LLM                    | Google Gemini (gemini-1.5-flash) |
| NLP Framework          | LangChain                        |
| Vector Database        | FAISS                            |
| Embeddings             | HuggingFace (MiniLM)             |
| PDF Processing         | PyPDF2                           |
| Environment Management | python-dotenv                    |




**ğŸ“¦ Dependencies**

streamlit
google-generativeai
python-dotenv
langchain
langchain-community
langchain-google-genai
PyPDF2
faiss-cpu
sentence-transformers



**ğŸ—ï¸ Project Architecture**

â”œâ”€â”€ app.py
â”œâ”€â”€ faiss_index/
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md




**ğŸ”„ Application Workflow**


1ï¸âƒ£ PDF Upload

Users upload one or more PDF files through the Streamlit sidebar.

2ï¸âƒ£ Text Extraction

PDFs are read using PyPDF2

Text is extracted page by page

3ï¸âƒ£ Text Chunking

Large text is split into chunks

Uses RecursiveCharacterTextSplitter

Ensures context is preserved using overlap

4ï¸âƒ£ Embedding Generation

Uses HuggingFace sentence-transformers

Model: all-MiniLM-L6-v2

Embeddings are normalized for better similarity search

5ï¸âƒ£ Vector Storage

FAISS stores embeddings locally

Enables fast semantic search

6ï¸âƒ£ Question Answering

User question â†’ similarity search

Relevant chunks passed to Gemini LLM

Context-aware response generated


**ğŸ§  Core Components Explained**

ğŸ”¹ get_pdf_text()

Extracts text from uploaded PDF files safely.

ğŸ”¹ get_text_chunks()

Splits large text into manageable overlapping chunks to preserve meaning.

ğŸ”¹ get_vectorstore()

Creates and saves a FAISS vector index using HuggingFace embeddings.

ğŸ”¹ get_conversational_chain()

Defines a custom prompt and initializes the Gemini LLM using LangChain.

ğŸ”¹ user_input()

Performs similarity search

Passes documents + question to LLM

Stores conversation history



**ğŸ” Environment Setup**

Create a .env file:

GOOGLE_API_KEY=your_google_gemini_api_key

**â–¶ï¸ How to Run the Project**

Step 1: Install Dependencies

pip install -r requirements.txt


Step 2: Run Streamlit App

streamlit run app.py


Step 3: Use the App

Upload PDF files

Click Submit & Process

Ask questions from the document content

ğŸ“¸ UI Preview

Main Chat Window for Q&A

Sidebar for PDF uploads

Chat history maintained across interactions

ğŸ“ˆ Use Cases

Research paper analysis

Legal document review

Study notes Q&A

Resume or report analysis

Enterprise document intelligence

**ğŸ§ª Future Improvements**

Multi-PDF source citation

Streaming responses

Authentication system

Cloud-based vector storage

Chat memory using LangChain Memory





